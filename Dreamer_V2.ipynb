{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPtwB+iDD52wscRCONRBSuS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seki-shu/Group15_Multimodal/blob/Dreamer-V2/Dreamer_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 準備"
      ],
      "metadata": {
        "id": "TfdWO2_f_ZwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## libraryのimport"
      ],
      "metadata": {
        "id": "0GIGUoGO-vBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.distributions as td\n",
        "from torch.distributions import Normal, Categorical, OneHotCategorical, OneHotCategoricalStraightThrough\n",
        "from torch.distributions import kl_divergence"
      ],
      "metadata": {
        "id": "6IyU5N79-2YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## deviceの確認"
      ],
      "metadata": {
        "id": "eKy-1uif_TRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.deviceを定義．この変数は後々モデルやデータをGPUに転送する時にも使います\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ogLmXvsB_gPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデルの実装\n"
      ],
      "metadata": {
        "id": "OpdMlxUpvtI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RSSM"
      ],
      "metadata": {
        "id": "Ucy9NSR8vxgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RSSM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mlp_hidden_dim: int,\n",
        "        h_dim: int,\n",
        "        z_dim: int,\n",
        "        a_dim: int,\n",
        "        n_classes: int,\n",
        "        embedding_dim: int\n",
        "    )\n",
        "    super().__init__()\n",
        "\n",
        "    self.h_dim = h_dim # 決定的状態. stoch\n",
        "    self.z_dim = z_dim # 確率的状態(カテゴリカル変数). deter\n",
        "    self.a_dim = a_dim # 行動\n",
        "    self.n_classes = n_classes # zのカテゴリ数\n",
        "\n",
        "    # Recurrent model\n",
        "    # h_t = f(h_t-1, z_t-1, a_t-1)\n",
        "    self.z_a_hidden = nn.Linear(z_dim * n_classes + a_dim, mlp_hidden_dim)\n",
        "    self.rnn = nn.GRUCell(mlp_hidden_dim, h_dim)\n",
        "\n",
        "    # Prior prediction\n",
        "    # z_t+1_hat = f(h_t+1)\n",
        "    self.prior_hidden = nn.Linear(h_dim, mlp_hidden_dim)\n",
        "    self.prior_logits = nn.Linear(mlp_hidden_dim, z_dim * n_classes)\n",
        "\n",
        "    # Posterior\n",
        "    # z_t+1 = f(h_t+1, o_t+1)\n",
        "    self.posterior_hidden = nn.Linear(h_dim + embedding_dim, mlp_hidden_dim) # competition_baselineのほうではembedding_dimがなぜかハードコーディングされてた\n",
        "    self.posterior_logits = nn.Linear(mlp_hidden_dim, z_dim * n_classes)\n",
        "\n",
        "    def recurrent(\n",
        "        self,\n",
        "        h_prev: torch.Tensor,\n",
        "        z_prev: torch.Tensor,\n",
        "        a_prev: torch.Tensor,\n",
        "        rnn_hidden: torch.Tensor\n",
        "    ):\n",
        "        \"\"\"\n",
        "        h_t = f(h_t-1, z_t-1, a_t-1)\n",
        "        \"\"\"\n",
        "        mlp_hidden = F.elu(self.z_a_hidden(torch.cat([z_prev, a_prev], dim=1)))\n",
        "        h = self.rnn(mlp_hidden, rnn_hidden)\n",
        "        return h\n",
        "\n",
        "    def get_prior(\n",
        "        self,\n",
        "        h: torch.Tensor,\n",
        "        detach = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        z_t+1_hat = f(h_t+1)\n",
        "        \"\"\"\n",
        "        mlp_hidden = F.elu(self.prior_hidden(h))\n",
        "        logits = self.prior_logits(mlp_hidden) # (B, z_dim * n_classes,)\n",
        "        logits = logits.reshape(logits.shape[0], self.z_dim, self.n_classes) # (B, z_dim, n_classes)\n",
        "\n",
        "        prior = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
        "        if detach:\n",
        "            detached_prior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n",
        "            return prior, detached_prior\n",
        "        return prior\n",
        "\n",
        "    def get_posterior(\n",
        "        self,\n",
        "        h: torch.Tensor,\n",
        "        embedded_obs: torch.Tensor,\n",
        "        detach = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        z_t+1 = f(h_t+1, o_t+1)\n",
        "        \"\"\"\n",
        "        mlp_hidden = F.elu(self.posterior_hidden(torch.cat([h, embedded_obs], dim=1)))\n",
        "        logits = self.posterior_logits(mlp_hidden) # (B, z_dim * n_classes)\n",
        "        logits = logits.reshape(logits.shape[0], self.z_dim, self.n_classes) # (B, z_dim, n_classes)\n",
        "\n",
        "        posterior = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
        "        if detach:\n",
        "            detached_posterior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n",
        "            return posterior, detached_posterior\n",
        "        return posterior"
      ],
      "metadata": {
        "id": "zEnpQhwuv2ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    (3, 64, 64) -> (1024, ) にエンコード\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        obs: torch.Tensor\n",
        "    ):\n",
        "        \"\"\"\n",
        "        順伝播を行うメソッド．観測画像をベクトルに埋め込む．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        obs : torch.Tensor (batch size, 3, 64, 64)\n",
        "            環境から得られた観測画像．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        embedded_obs : torch.Tensor (batch size, 1024)\n",
        "            観測を1024次元のベクトルに埋め込んだもの．\n",
        "        \"\"\"\n",
        "        embed = F.elu(self.conv1(obs))\n",
        "        embed = F.elu(self.conv2(embed))\n",
        "        embed = F.elu(self.conv3(embed))\n",
        "        embed = F.elu(self.conv4(embed)).reshape(embed.shape[0], -1)\n",
        "        return embed"
      ],
      "metadata": {
        "id": "B5f9vR5_6YrW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}