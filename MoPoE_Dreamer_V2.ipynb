{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNtIUqtCOqFurIsvj/FxPbt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seki-shu/Group15_Multimodal/blob/Dreamer-V2/MoPoE_Dreamer_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 準備"
      ],
      "metadata": {
        "id": "TfdWO2_f_ZwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "0GIGUoGO-vBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.distributions as td\n",
        "from torch.distributions import Normal, Categorical, OneHotCategorical, OneHotCategoricalStraightThrough, Bernoulli\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "6IyU5N79-2YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check device"
      ],
      "metadata": {
        "id": "eKy-1uif_TRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.deviceを定義．この変数は後々モデルやデータをGPUに転送する時にも使います\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ogLmXvsB_gPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TrainedModels"
      ],
      "metadata": {
        "id": "-CxOnj3kzOe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainedModels:\n",
        "    def __init__(self, *models):\n",
        "        \"\"\"\n",
        "        コンストラクタ.\n",
        "        使用例: trained_models = TrainedModels(encoder, rssm, critic, actor)\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            models : nn.Module\n",
        "                保存するモデル. 複数選択可.\n",
        "        \"\"\"\n",
        "        assert np.all([nn.Module in model.__class__.__bases__ for model in models]), \"models must be nn.Module\"\n",
        "        self.models = models\n",
        "\n",
        "    def save(self, dir: str):\n",
        "        \"\"\"\n",
        "        モデルを保存するメソッド.\n",
        "        コンストラクタで渡したモデルをすべて保存する.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            dir : str\n",
        "                保存先ディレクトリ.\n",
        "        \"\"\"\n",
        "        for model in self.models:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(dir, f\"{model.__class__.__name__}.pt\")\n",
        "            )\n",
        "\n",
        "    def load(self, dir: str, device: str):\n",
        "        \"\"\"\n",
        "        モデルを読み込むメソッド.\n",
        "        ※モデルの名前とファイルの名前は同じにする.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            dir : str\n",
        "                読み込むディレクトリ.\n",
        "            device : str\n",
        "                読み込むデバイス. 'cpu', 'cuda'\n",
        "        \"\"\"\n",
        "        for model in self.models:\n",
        "            model.load_state_dict(\n",
        "                torch.load(\n",
        "                    os.path.join(dir, f\"{model.__class__.__name__}.pt\"),\n",
        "                    map_location=device\n",
        "                )\n",
        "            )"
      ],
      "metadata": {
        "id": "H8KvY313zS7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## set_seed"
      ],
      "metadata": {
        "id": "54_T5pFN-uTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Pytorchとnumpyのseed値を固定する.\n",
        "\n",
        "    Params:\n",
        "    -------\n",
        "        seed : int\n",
        "          シード値.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "GhE5VP-Z-z4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデルの実装\n"
      ],
      "metadata": {
        "id": "OpdMlxUpvtI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RSSM"
      ],
      "metadata": {
        "id": "Ucy9NSR8vxgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RSSM(nn.Module):\n",
        "    \"\"\"\n",
        "    RSSMのクラス.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "        recurrent: 決定的状態hをrnnで得る.\n",
        "        get_prior: 確率的状態zの事前分布を得る.\n",
        "        get_posterior: 確率的状態zの事後分布を得る.\n",
        "    \"\"\"\n",
        "    def __init__(self, mlp_hidden_dim: int, h_dim: int, z_dim: int, a_dim: int, n_classes: int, embedding_dim: int):\n",
        "        \"\"\"\n",
        "        コンストラクタ.\n",
        "\n",
        "        Params:\n",
        "        --------\n",
        "            mlp_hidden_dim : int\n",
        "               mlpに通した後の次元数.\n",
        "            h_dim : int\n",
        "               決定的状態hの次元数.\n",
        "            z_dim : int\n",
        "               確率的状態zの次元数.\n",
        "            a_dim : int\n",
        "               行動の次元数.\n",
        "            n_classes : int\n",
        "               確率的状態zのカテゴリ数.\n",
        "            embedding_dim : int\n",
        "               観測画像の埋め込み次元数.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "        self.a_dim = a_dim\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Recurrent model\n",
        "        # h_t = f(h_t-1, z_t-1, a_t-1)\n",
        "        self.z_a_hidden = nn.Linear(z_dim * n_classes + a_dim, mlp_hidden_dim)\n",
        "        self.rnn = nn.GRUCell(mlp_hidden_dim, h_dim)\n",
        "\n",
        "        # Prior prediction\n",
        "        # z_t+1_hat = f(h_t+1)\n",
        "        self.prior_hidden = nn.Linear(h_dim, mlp_hidden_dim)\n",
        "        self.prior_logits = nn.Linear(mlp_hidden_dim, z_dim * n_classes)\n",
        "\n",
        "        # Posterior\n",
        "        # z_t+1|x_1 = f(h_t+1, o_t+1)\n",
        "        self.posterior_1_hidden = nn.Linear(h_dim + embedding_dim, mlp_hidden_dim)\n",
        "        self.posterior_1_logits = nn.Linear(mlp_hidden_dim, z_dim * n_classes)\n",
        "        # z_t+1|x_2\n",
        "        self.posterior_2_hidden = nn.Linear(h_dim + embedding_dim, mlp_hidden_dim)\n",
        "        self.posterior_2_logits = nn.Linear(mlp_hidden_dim, z_dim * n_classes)\n",
        "        # z_t+1|x_3\n",
        "        self.posterior_3_hidden = nn.Linear(h_dim + embedding_dim, mlp_hidden_dim)\n",
        "        self.posterior_3_logits = nn.Linear(mlp_hidden_dim, z_dim * n_classes)\n",
        "\n",
        "\n",
        "\n",
        "    def recurrent(self, h_prev: torch.Tensor, z_prev: torch.Tensor, a_prev: torch.Tensor):\n",
        "        \"\"\"\n",
        "        決定的状態の状態遷移を求めるメソッド. RNNとしてGRUを使用する.\n",
        "        h_t+1 = f(h_t, z_t, a_t)\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            h_prev : torch.Tensor (batch size, h_dim)\n",
        "                現在時刻の決定的状態h_t\n",
        "            z_prev : torch.Tensor (batch size, z_dim)\n",
        "                現在時刻の確率的状態z_t\n",
        "            a_prev : torch.Tensor (batch size, a_dim)\n",
        "                現在時刻のアクションa_t\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "            h : torch.Tensor (batch size, h_dim)\n",
        "                次時刻の決定的状態h_t+1\n",
        "        \"\"\"\n",
        "        mlp_hidden = F.elu(self.z_a_hidden(torch.cat([z_prev, a_prev], dim=1)))\n",
        "        h = self.rnn(mlp_hidden, mlp_hidden)\n",
        "        return h\n",
        "\n",
        "    def get_prior(self,h: torch.Tensor, detach = False):\n",
        "        \"\"\"\n",
        "        確率的状態zの事前分布を求めるメソッド.\n",
        "        z_t+1_hat ~ p(h_t+1)\n",
        "\n",
        "        Params:\n",
        "            h : torch.Tensor (batch size, h_dim)\n",
        "                次時刻の決定的状態h_t+1.\n",
        "            detach : bool\n",
        "                Trueの場合、detachする.\n",
        "\n",
        "        ---------\n",
        "        Returns:\n",
        "            prior : torch.distributions.Distribution\n",
        "                事前分布を求めるためのカテゴリカル分布.\n",
        "            detached_prior : torch.distributions.Independent\n",
        "                detachされた事前分布を求めるためのカテゴリカル分布.\n",
        "        \"\"\"\n",
        "        mlp_hidden = F.elu(self.prior_hidden(h))\n",
        "        logits = self.prior_logits(mlp_hidden) # (B, z_dim * n_classes,)\n",
        "        logits = logits.reshape(logits.shape[0], self.z_dim, self.n_classes) # (B, z_dim, n_classes)\n",
        "\n",
        "        prior = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
        "        if detach:\n",
        "            detached_prior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n",
        "            return prior, detached_prior\n",
        "        return prior\n",
        "\n",
        "    def get_posterior(self, h: torch.Tensor, embedded_obs: torch.Tensor, detach = False):\n",
        "        \"\"\"\n",
        "        確率的状態zの事後分布を求めるメソッド.\n",
        "        MoE( PoE( z_t+1 ~ p(h_t+1, o_t+1) ) )\n",
        "\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            h : torch.Tensor (batch size, h_dim)\n",
        "                次時刻の決定的状態h_t+1.\n",
        "            embedded_obs_1 : torch.Tensor (batch size, embedding_dim)\n",
        "                次時刻の画像の埋め込み\n",
        "            detach : bool\n",
        "                Trueの場合、detachする.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "            posterior : torch.distributions.Distribution\n",
        "                事後分布を求めるためのカテゴリカル分布.\n",
        "            detached_posterior : torch.distributions.Independent\n",
        "                detachされた事後分布を求めるためのカテゴリカル分布.\n",
        "        \"\"\"\n",
        "        mlp_hidden = F.elu(self.posterior_hidden(torch.cat([h, embedded_obs], dim=1)))\n",
        "        logits = self.posterior_logits(mlp_hidden) # (B, z_dim * n_classes)\n",
        "        logits = logits.reshape(logits.shape[0], self.z_dim, self.n_classes) # (B, z_dim, n_classes)\n",
        "\n",
        "        posterior = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
        "        if detach:\n",
        "            detached_posterior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n",
        "            return posterior, detached_posterior\n",
        "        return posterior"
      ],
      "metadata": {
        "id": "zEnpQhwuv2ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "WMago0IsS1Dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    ※画像(1, 64, 64)をエンコードする.\n",
        "    画像はカラーじゃなくてグレースケールにするのでチャネル数が1.\n",
        "\n",
        "    Methods:\n",
        "    -------\n",
        "        forward: 観測画像を埋め込む.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 48, kernel_size=4, stride=2)\n",
        "        self.conv2 = nn.Conv2d(48, 96, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(96, 192, kernel_size=4, stride=2)\n",
        "        self.conv4 = nn.Conv2d(192, 384, kernel_size=4, stride=2)\n",
        "\n",
        "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        順伝播を行うメソッド．観測画像をベクトルに埋め込む．\n",
        "        (1, 64, 64) -> (1536, ) にエンコード.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            obs : torch.Tensor (batch size, 1, 64, 64)\n",
        "                環境から得られた観測画像．\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "            embed : torch.Tensor (batch size, 1536)\n",
        "                観測を1024次元のベクトルに埋め込んだもの．\n",
        "        \"\"\"\n",
        "        embed = F.elu(self.conv1(obs))\n",
        "        embed = F.elu(self.conv2(embed))\n",
        "        embed = F.elu(self.conv3(embed))\n",
        "        embed = self.conv4(embed).reshape(embed.shape[0], -1)\n",
        "        return embed"
      ],
      "metadata": {
        "id": "B5f9vR5_6YrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "3CrHwaS6S3fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    内部状態表現h, zから画像(1, 64, 64)を再構成する.\n",
        "    ただし画像は平均を計算して、その平均で分散１の正規分布から得る.\n",
        "\n",
        "    Methods:\n",
        "        forward: 画像の再構成を行う.\n",
        "    \"\"\"\n",
        "    def __init__(self, h_dim: int, z_dim: int, n_classes: int):\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Params:\n",
        "        --------\n",
        "            h_dim : int\n",
        "                決定的状態hの次元数．\n",
        "            z_dim : int\n",
        "                確率的状態zの次元数．\n",
        "            n_classes : int\n",
        "                確率的状態zのカテゴリ数．\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(h_dim + z_dim * n_classes, 1536)\n",
        "        self.deconv1 = nn.ConvTranspose2d(1536, 192, kernel_size=5, stride=2)\n",
        "        self.deconv2 = nn.ConvTranspose2d(192, 96, kernel_size=5, stride=2)\n",
        "        self.deconv3 = nn.ConvTranspose2d(96, 48, kernel_size=6, stride=2)\n",
        "        self.deconv4 = nn.ConvTranspose2d(48, 1, kernel_size=6, stride=2)\n",
        "\n",
        "    def forward(self, h: torch.Tensor, z: torch.Tensor):\n",
        "        \"\"\"\n",
        "        順伝播を行うメソッド．内部状態h, zから画像を再構成する．\n",
        "        mean = f(h_t, z_t)\n",
        "        o_t = N(mean, 1)\n",
        "\n",
        "        Params:\n",
        "        --------\n",
        "            h : torch.Tensor (batch size, h_dim)\n",
        "                決定的状態h\n",
        "            z : torch.Tensor (batch size, z_dim * n_classes)\n",
        "                確率的状態z\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "            obs_dist : torch.distributions.Independent\n",
        "                再構成された画像を得るための多変量正規分布．\n",
        "        \"\"\"\n",
        "        x = self.fc(torch.cat([h, z], dim=1))\n",
        "        x = x.reshape(x.shape[0], 1536, 1, 1)\n",
        "        x = F.elu(self.deconv1(x))\n",
        "        x = F.elu(self.deconv2(x))\n",
        "        x = F.elu(self.deconv3(x))\n",
        "        mean = self.deconv4(x)\n",
        "        obs_dist = td.Independent(Normal(mean, 1), 3)\n",
        "        return obs_dist\n"
      ],
      "metadata": {
        "id": "3wS4h5LRS5ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reward Model"
      ],
      "metadata": {
        "id": "NqLaH-bVcdAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardModel(nn.Module):\n",
        "    \"\"\"\n",
        "    報酬予測モデル.\n",
        "    ただし報酬は平均を計算して、その平均で分散が1の正規分布から得る.\n",
        "\n",
        "    Methods:\n",
        "    -------\n",
        "        forward: 報酬予測を行う.\n",
        "    \"\"\"\n",
        "    def __init__(self, h_dim: int, z_dim: int, n_classes: int, hidden_dim: int):\n",
        "        \"\"\"\n",
        "        コンストラクタ.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            h_dim : int\n",
        "                決定的状態hの次元数.\n",
        "            z_dim : int\n",
        "                確率的状態zの次元数.\n",
        "            n_classes : int\n",
        "                確率的状態zのカテゴリ数.\n",
        "            hidden_dim : int\n",
        "                MLPを通した後のユニット数.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(h_dim + z_dim * n_classes, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, h: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        報酬予測を行うメソッド.\n",
        "        mean = f(h_t, z_t)\n",
        "        r_t = N(mean, 1)\n",
        "\n",
        "        Params:\n",
        "        ----------\n",
        "            h : torch.Tensor (batch size, h_dim)\n",
        "                決定的状態h\n",
        "            z : torch.Tensor (batch size, z_dim * n_classes)\n",
        "                確率的状態z\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "            reward_dist : torch.distributions.Independent\n",
        "                報酬予測をするための多変量正規分布.\n",
        "        \"\"\"\n",
        "        x = F.elu(self.fc1(torch.cat([h, z], dim=1)))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        mean = self.fc4(x)\n",
        "        reward_dist = td.Independent(Normal(mean, 1), 1)\n",
        "        return reward_dist"
      ],
      "metadata": {
        "id": "kT1UTPlFcfuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discount Model\n"
      ],
      "metadata": {
        "id": "ulP85WUBKp4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    現在のエピソードが終端かどうかを求めるモデル.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "        forward: 終端かどうか予測を行う.\n",
        "    \"\"\"\n",
        "    def __init__(self, h_dim: int, z_dim: int, n_classes: int, hidden_dim: int):\n",
        "        \"\"\"\n",
        "        コンストラクタ.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            h_dim : int\n",
        "                決定的状態hの次元数.\n",
        "            z_dim : int\n",
        "                確率的状態zの次元数.\n",
        "            n_classes : int\n",
        "                確率的状態zのカテゴリ数.\n",
        "            hidden_dim : int\n",
        "                MLPを通した後のユニット数.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(h_dim + z_dim * n_classes, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, h: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        現在のエピソードが終端かどうか求める.\n",
        "        p = f(h_t, z_t)\n",
        "        gamma_t = Bernoulli(p)\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            h : torch.Tensor (batch size, h_dim)\n",
        "                決定的状態h\n",
        "            z : torch.Tensor (batch size, z_dim * n_classes)\n",
        "                確率的状態z\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "            discount_dist : torch.distributions.Independent\n",
        "                終端かどうか予測を行うベルヌーイ分布\n",
        "        \"\"\"\n",
        "        x = F.elu(self.fc1(torch.cat([h, z], dim=1)))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        p_logits = self.fc4(x)\n",
        "        discount_dist = td.Independent(Bernoulli(logits=p_logits), 1)\n",
        "        return discount_dist"
      ],
      "metadata": {
        "id": "50mWXQbCKt6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor"
      ],
      "metadata": {
        "id": "sc3et5bZPUEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    \"\"\"\n",
        "    最適行動を出力するモデル.\n",
        "    今回はAtariではないので連続な行動を出力する.\n",
        "\n",
        "    Methods:\n",
        "    -------\n",
        "        forward: 最適行動を出力する.\n",
        "    \"\"\"\n",
        "    def __init__(self, action_dim: int, h_dim: int, z_dim: int, n_classes: int, hidden_dim: int):\n",
        "        \"\"\"\n",
        "        コンストラクタ.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            action_dim : int\n",
        "                行動の次元数.\n",
        "            h_dim : int\n",
        "                決定的状態hの次元数.\n",
        "            z_dim : int\n",
        "                確率的状態zの次元数.\n",
        "            n_classes : int\n",
        "                確率的状態zのカテゴリ数.\n",
        "            hidden_dim : int\n",
        "                MLPを通した後のユニット数.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(h_dim + z_dim * n_classes, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_mean = nn.Linear(hidden_dim, action_dim)\n",
        "        self.fc_std = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, h: torch.Tensor, z: torch.Tensor, eval: bool):\n",
        "        \"\"\"\n",
        "        最適行動を出力する.\n",
        "        a_t = f(h_t, z_t)\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            h : torch.Tensor (batch size, h_dim)\n",
        "                決定的状態h.\n",
        "            z : torch.Tensor (batch size, z_dim * n_classes)\n",
        "                確率的状態z.\n",
        "            eval : bool\n",
        "                評価モードかどうか.\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "            actions : torch.Tensor (batch size, action_dim)\n",
        "                最適行動.\n",
        "            action_log_probs : torch.Tensor (batch size, 1)\n",
        "                最適行動の対数尤度.\n",
        "            action_entropy : torch.Tensor (batch size, 1)\n",
        "                最適行動のエントロピー.\n",
        "        \"\"\"\n",
        "        x = F.elu(self.fc1(torch.cat([h, z], dim=1)))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        mean = self.fc_mean(x) # (batch_size, action_dim)\n",
        "        std = F.softplus(self.fc_std(x))\n",
        "\n",
        "        if eval:\n",
        "            actions = torch.tanh(mean) # action spaceを(-1, 1)にしてる. 環境によっては変更必要かも.\n",
        "            return actions, None, None\n",
        "\n",
        "        # Dreamer V2の連続空間でのActorがどうなっているかいまいち分からなかったので、とりあえず以下のように実装しました。\n",
        "        # まずactionを正規分布からサンプルした後、tanh関数に通して(-1, 1)にします。\n",
        "        # 対数尤度を求めるときに、log p(actions) = log Normal(unscaled_actions) * |det(∂unscaled_actions / ∂actions)|(https://arxiv.org/pdf/1801.01290 Appendix C)となります\n",
        "        # tanhの微分は1-tanh^2で、正規分布の分散共分散行列は対角行列なので、ヤコビアンはdiag(1 - tanh^2)となります。\n",
        "        # 間違ってるかもしれません...\n",
        "        action_dist = td.Independent(Normal(mean, std), 1)\n",
        "        unscaled_actions = action_dist.rsample()\n",
        "        actions = torch.tanh(unscaled_actions)\n",
        "\n",
        "        action_log_probs = action_dist.log_prob(unscaled_actions) - torch.log(1 - actions.pow(2) + 1e-7).sum(dim=1, keepdim=True)\n",
        "        action_entropy = action_dist.entropy()\n",
        "        return actions, action_log_probs, action_entropy"
      ],
      "metadata": {
        "id": "m6cwAbsdPWlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Critic"
      ],
      "metadata": {
        "id": "GarQGZjT0nNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    \"\"\"\n",
        "    状態価値を出力するモデル.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "        forward: 状態価値を出力する.\n",
        "    \"\"\"\n",
        "    def __init__(self, h_dim: int, z_dim: int, n_classes: int, hidden_dim: int):\n",
        "        \"\"\"\n",
        "        コンストラクタ.\n",
        "\n",
        "        params:\n",
        "        -------\n",
        "            h_dim : int\n",
        "                決定的状態hの次元数.\n",
        "            z_dim : int\n",
        "                確率的状態zの次元数.\n",
        "            n_classes : int\n",
        "                確率的状態zのカテゴリ数.\n",
        "            hidden_dim : int\n",
        "                MLPを通した後のユニット数.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(h_dim + z_dim * n_classes, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, h: torch.Tensor, z: torch.Tensor):\n",
        "        \"\"\"\n",
        "        状態価値を出力する.\n",
        "        v_t = f(h_t, z_t)\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            h : torch.Tensor (batch size, h_dim)\n",
        "                決定的状態h.\n",
        "            z : torch.Tensor (batch size, z_dim * n_classes)\n",
        "                確率的状態z.\n",
        "\n",
        "        Return:\n",
        "        -------\n",
        "            values : torch.Tensor (batch size, 1)\n",
        "                状態価値.\n",
        "        \"\"\"\n",
        "        x = F.elu(self.fc1(torch.cat([h, z], dim=1)))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        values = self.out(x)\n",
        "        return values"
      ],
      "metadata": {
        "id": "Ywfm04w10piP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# その他機能"
      ],
      "metadata": {
        "id": "mPznwrXrB-wD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay Buffer\n"
      ],
      "metadata": {
        "id": "parOAYbvCCP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    リプレイバッファ.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity: int, obs_shape: tuple[int], action_dim: int):\n",
        "        \"\"\"\n",
        "        コンストラクタ.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            capacity : int\n",
        "                バッファの最大容量.\n",
        "            obs_shape : tuple[int]\n",
        "                観測の形状.\n",
        "            action_dim : int\n",
        "                行動の次元数.\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.obs_shape = obs_shape\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        self.observations = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
        "        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n",
        "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
        "        self.dones = np.zeros((capacity, 1), dtype=bool)\n",
        "        self.index = 0\n",
        "        self.is_filled = False\n",
        "\n",
        "    def push(self, observation: np.ndarray, action: np.ndarray, reward: float, done: bool):\n",
        "        \"\"\"\n",
        "        バッファに追加する.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            observation : np.ndarray (obs_shape)\n",
        "                観測.\n",
        "            action : np.ndarray (action_dim)\n",
        "                行動.\n",
        "            reward : float\n",
        "                報酬.\n",
        "            done : bool\n",
        "                エピソードの終了.\n",
        "        \"\"\"\n",
        "        self.observations[self.index] = observation\n",
        "        self.actions[self.index] = action\n",
        "        self.rewards[self.index] = reward\n",
        "        self.dones[self.index] = done\n",
        "\n",
        "        if self.index == self.capacity - 1:\n",
        "            self.is_filled = True\n",
        "        self.index = (self.index + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size: int, chunk_length: int) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        バッファからサンプリングする.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            batch_size : int\n",
        "                サンプリングするデータの数.\n",
        "\n",
        "            chunk_length : int\n",
        "                chunkの長さ\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "            sampled_observations : np.ndarray (B, T, 1, 64, 64)\n",
        "                サンプリングされた観測.\n",
        "            sampled_actions : np.ndarray (B, T, action_dim)\n",
        "                サンプリングされた行動.\n",
        "            sampled_rewards : np.ndarray (B, T, 1)\n",
        "                サンプリングされた報酬.\n",
        "            sampled_dones : np.ndarray (B, T, 1)\n",
        "                サンプリングされたエピソードの終了判定.\n",
        "        \"\"\"\n",
        "        episode_borders = np.where(self.dones)[0] # episodeが終了するindex\n",
        "        sampled_indices = []\n",
        "        for _ in range(batch_size):\n",
        "            crossed_border = True # borderを跨いだかどうか\n",
        "            while crossed_border:\n",
        "                initial_idx = np.random.randint(self.capacity - chunk_length + 1)\n",
        "                final_idx = initial_idx + chunk_length - 1\n",
        "                crossed_border = np.logical_and(initial_idx <= episode_borders,\n",
        "                                                episode_borders < final_idx).any()\n",
        "            sampled_indices += list(range(initial_idx, final_idx + 1)) # 最終的に(batch_size * chunk_length)\n",
        "\n",
        "        sampled_observations = self.observations[sampled_indices].reshape(\n",
        "            batch_size, chunk_length, *self.obs_shape)\n",
        "        sampled_actions = self.actions[sampled_indices].reshape(\n",
        "            batch_size, chunk_length, self.action_dim)\n",
        "        sampled_rewards = self.rewards[sampled_indices].reshape(\n",
        "            batch_size, chunk_length, 1)\n",
        "        sampled_dones = self.dones[sampled_indices].reshape(\n",
        "            batch_size, chunk_length, 1)\n",
        "        return sampled_observations, sampled_actions, sampled_rewards, sampled_dones\n",
        "\n",
        "    def save(self, dir: str):\n",
        "        np.save(f\"{dir}/observations.npy\", self.observations)\n",
        "        np.save(f\"{dir}/actions.npy\", self.actions)\n",
        "        np.save(f\"{dir}/rewards.npy\", self.rewards)\n",
        "        np.save(f\"{dir}/dones.npy\", self.dones)\n",
        "\n",
        "    def load(self, dir: str):\n",
        "        self.observations = np.load(f\"{dir}/observations.npy\")\n",
        "        self.actions = np.load(f\"{dir}/actions.npy\")\n",
        "        self.rewards = np.load(f\"{dir}/rewards.npy\")\n",
        "        self.dones = np.load(f\"{dir}/dones.npy\")\n"
      ],
      "metadata": {
        "id": "7RSO7DksDr36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## calculate λ-target  \n",
        "元論文ではλ: 0.95, H=15  \n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "tk0BYp-bDWFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_lambda_target(rewards: torch.Tensor, discounts: torch.Tensor, values: torch.Tensor, lambda_: float):\n",
        "    \"\"\"\n",
        "    λ-targetを計算する.\n",
        "\n",
        "    Params:\n",
        "    -------\n",
        "        rewards : torch.Tensor (imagination_horizon, D)\n",
        "            報酬. 1次元目が時刻tを表しており、2次元目は自由な変数で、想像の軌道を作成するとき入力されるindex.\n",
        "        discounts : torch.Tensor (imagination_horizon, D)\n",
        "            割引率.\n",
        "        values : torch.Tensor (imagination_horizon, D)\n",
        "            状態価値.\n",
        "        lambda_ : float\n",
        "            λ. 元論文ではλ=0.95\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "        V_lambda : torch.Tensor (imagination_horizon, D)\n",
        "            lambda_targetの値.\n",
        "    \"\"\"\n",
        "    V_lambda = torch.zeros_like(rewards)\n",
        "\n",
        "    for t in reversed(range(rewards.shape[0])):\n",
        "        if t == rewards.shape[0] - 1: # t = H(下の条件式)\n",
        "            V_lambda[t] = rewards[t] + discounts[t] * values[t]\n",
        "        else:\n",
        "            V_lambda[t] = rewards[t] + discounts[t] * ((1 - lambda_) * values[t+1] + lambda_ * V_lambda[t+1])\n",
        "\n",
        "    return V_lambda"
      ],
      "metadata": {
        "id": "ZkmfL-4uDdUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess obs"
      ],
      "metadata": {
        "id": "3vPU_vqHqeg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_obs(obs):\n",
        "    \"\"\"\n",
        "    画像の変換. [0, 255] -> [0, 1]\n",
        "    画像をグレースケールに変換\n",
        "    \"\"\"\n",
        "    height, width = obs.shape[0], obs.shape[1]\n",
        "    obs = Image.fromarray(obs)\n",
        "    obs = obs.convert(\"L\") # grayscaleにしてる\n",
        "    obs = np.array(obs).reshape(height, width, 1)\n",
        "    obs = obs.astype(np.float32) / 255.0 - 0.5\n",
        "    return obs"
      ],
      "metadata": {
        "id": "iH23yi9Fqjm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 学習"
      ],
      "metadata": {
        "id": "n8TwwNW_pOR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "Xz-wri6oL_fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    Agentのクラス.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "        __call__: Agentが一連の流れを行う.\n",
        "        reset: 決定的状態hのリセット.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, rssm, actor):\n",
        "        self.encoder = encoder\n",
        "        self.rssm = rssm\n",
        "        self.actor = actor\n",
        "        self.device = next(self.actor.parameters()).device\n",
        "        self.h = torch.zeros(1, rssm.h_dim, device=self.device)\n",
        "\n",
        "    def __call__(self, obs, eval=False):\n",
        "        \"\"\"\n",
        "        preprocessを適用＋Channel-Firstに変換.\n",
        "        その後画像をエンコードして事後分布を取得.\n",
        "        事後分布とhから行動を選択.\n",
        "        最後に事後分布と行動を基にhを更新.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "            obs : np.ndarray (64, 64, 3)だと思う.\n",
        "                環境から得られた観測画像.(preprocess前).\n",
        "            eval : bool\n",
        "                訓練か評価時か\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "            action : np.ndarray (action_dim, )のはず.\n",
        "                観測画像に対する行動の予測.\n",
        "        \"\"\"\n",
        "        obs = preprocess_obs(obs)\n",
        "        obs = torch.as_tensor(obs, device=self.device)\n",
        "        obs.transpose(1, 2).transpose(0, 1).unsqueeze(0) # channel-firstに (1, C, H, W)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # 観測を低次元に変換し、得られたposteriorをActorに入れて行動を決定する.\n",
        "            embedded_obs = self.encoder(obs)\n",
        "            z_posterior_dist = self.rssm.get_posterior(self.h, embedded_obs)\n",
        "            z_posterior = z_posterior_dist.sample().flatten(1) # (1, z_dim, n_classes)を(1, z_dim * n_classes)にflatten\n",
        "            action, _, _ = self.actor(self.h, z_posterior, eval=eval) # 行動の取得 (1, action_dim)\n",
        "            self.h = self.rssm.recurrent(self.h, z_posterior, action) # hの更新\n",
        "\n",
        "        return action.squeeze().cpu().numpy() # (action_dim, )\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        隠れ状態のreset.\n",
        "        \"\"\"\n",
        "        self.h = torch.zeros(1, self.rssm.h_dim, device=self.device)\n"
      ],
      "metadata": {
        "id": "UBXFQiP7MBd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "EHNu7kzzhIyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self, **kwargs):\n",
        "        # data settings\n",
        "        self.buffer_size = 100_000\n",
        "        self.batch_size = 50\n",
        "        self.seq_length = 50\n",
        "        self.imagination_horizon = 15\n",
        "\n",
        "        # model dimensions\n",
        "        self.z_dim = 32\n",
        "        self.n_classes = 32\n",
        "        self.h_dim = 600\n",
        "        self.mlp_hidden_dim = 400\n",
        "\n",
        "        # learning parameters\n",
        "        self.model_lr = 2e-4 # world model(transition / prior / posterior / discount / image predictor)の学習率\n",
        "        self.actor_lr = 4e-5\n",
        "        self.critic_lr = 1e-4\n",
        "        self.epsilon = 1e-5 # optimizerのεの値\n",
        "        self.weight_decay = 1e-6\n",
        "        self.gradient_clipping = 100\n",
        "        self.kl_loss_scale = 0.1\n",
        "        self.kl_balance = 0.8\n",
        "        self.actor_entropy_scale = 1e-4 # 連続値制御の場合 Atariの場合は1e-3\n",
        "        self.slow_critic_update = 100\n",
        "        self.reward_loss_scale = 1.0\n",
        "        self.discount_loss_scale = 1.0\n",
        "        self.update_freq = 4\n",
        "\n",
        "        # lambda return params\n",
        "        self.discount = 0.995 # 割引率\n",
        "        self.lambda_ = 0.95\n",
        "\n",
        "        # learning period settings\n",
        "        self.seed_iter = 5_000 # 事前にランダム行動で探索する回数\n",
        "        self.eval_freq = 1e3\n",
        "        self.eval_episodes = 5\n",
        "\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "U2LbIFoIhGjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデル等の初期化"
      ],
      "metadata": {
        "id": "2IvkNYPFkM__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "set_seed(seed)\n",
        "# env =\n",
        "# eval_env =\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# action_dim = env.action_space.n\n",
        "\n",
        "#replay buffer\n",
        "replay_buffer = ReplayBuffer(\n",
        "    capacity = cfg.buffer_size,\n",
        "    obs_shape = (64, 64, 1),\n",
        "    # action_dim = env.action_space.n\n",
        ")\n",
        "\n",
        "#models\n",
        "rssm = RSSM(cfg.mlp_hidden_dim, cfg.h_dim, cfg.z_dim, action_dim, cfg.n_classes, 1536).to(device)\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder(cfg.h_dim, cfg.z_dim, cfg.n_classes).to(device)\n",
        "reward_model = RewardModel(cfg.h_dim, cfg.z_dim, cfg.n_classes, cfg.mlp_hidden_dim).to(device)\n",
        "discount_model = DiscountModel(cfg.h_dim, cfg.z_dim, cfg.n_classes, cfg.mlp_hidden_dim).to(device)\n",
        "actor = Actor(action_dim, cfg.h_dim, cfg.z_dim, cfg.n_classes, cfg.mlp_hidden_dim).to(device)\n",
        "critic = Critic(cfg.h_dim, cfg.z_dim, cfg.n_classes, cfg.mlp_hidden_dim).to(device)\n",
        "target_critic = Critic(cfg.h_dim, cfg.z_dim, cfg.n_classes, cfg.mlp_hidden_dim).to(device)\n",
        "target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "trained_models = TrainedModels(\n",
        "    rssm,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    reward_model,\n",
        "    discount_model,\n",
        "    actor,\n",
        "    critic\n",
        ")\n",
        "\n",
        "# optimizer\n",
        "wm_params = list(rssm.parameters())          + \\\n",
        "            list(encoder.parameters())       + \\\n",
        "            list(decoder.parameters())       + \\\n",
        "            list(reward_model.parameters())  + \\\n",
        "            list(discount_model.parameters())\n",
        "wm_optimizer = torch.optim.AdamW(wm_parameters, lr=cfg.model_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n",
        "actor_optimizer = torch.optim.AdamW(actor.parameters(), lr=cfg.actor_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n",
        "critic_optimizer = torch.optim.AdamW(critic.parameters(), lr=cfg.critic_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)"
      ],
      "metadata": {
        "id": "hpBCGSh2kPPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluation"
      ],
      "metadata": {
        "id": "c49jJb_9RFdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(eval_env: RepeatAction, policy: Agent, step: int, cfg: Config):\n",
        "    \"\"\"\n",
        "    評価用の関数.\n",
        "\n",
        "    Params:\n",
        "    -------\n",
        "        eval_env : RepeatAction\n",
        "            評価用の環境.\n",
        "        agent : Agent\n",
        "            Agentのインスタンス.\n",
        "        step : int\n",
        "            現在のステップ数.\n",
        "        cfg : Config\n",
        "            Config.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "        max_ep_rewards : float\n",
        "            評価中に1エピソードで得た最大の報酬和.\n",
        "    \"\"\"\n",
        "    env = eval_env\n",
        "    all_ep_rewards = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(cfg.eval_episodes):\n",
        "            obs = env.reset() # 環境をリセット\n",
        "            agent.reset() # 隠れ状態をリセット\n",
        "            done = False\n",
        "            episode_reward = 0 # エピソードでの報酬和\n",
        "            while not done:\n",
        "                action = agent(obs, eval=True) # (action_dim, )\n",
        "                obs, reward, done, _ = env.step(action) # 環境を進める\n",
        "                episode_reward += reward\n",
        "            all_ep_rewards.append(episode_reward)\n",
        "\n",
        "        mean_ep_rewards = np.mean(all_ep_rewards)\n",
        "        max_ep_rewards = np.max(all_ep_rewards)\n",
        "        # print(f\"Eval(iter={cfg.eval_episodes}) mean: {mean_ep_rewards:.4f} max: {max_ep_rewards:.4f}\")\n",
        "\n",
        "    return max_ep_rewards, mean_ep_rewards"
      ],
      "metadata": {
        "id": "P-EPuMaJRHzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ランダム行動でバッファを埋める"
      ],
      "metadata": {
        "id": "IAFkAXXKontA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()\n",
        "done = False\n",
        "for _ in range(cfg.seed_iter):\n",
        "    action = env.action_space.sample()\n",
        "    next_obs, reward, done, _ = env.step(action)\n",
        "    replay_buffer.push(preprocess_obs(obs), action, reward, done) # Atari環境じゃないのでrewardをtanhで[-1, 1]にしない.\n",
        "\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "\n",
        "    else:\n",
        "        obs = next_obs"
      ],
      "metadata": {
        "id": "YwJoUpWcoq74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 実際に学習を行う"
      ],
      "metadata": {
        "id": "yx0Rx7Y6xvdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(encoder, rssm, actor)\n",
        "# 学習曲線の表示\n",
        "log_dir = 'logs'\n",
        "writer = SummaryWriter(log_dir)\n",
        "% tensorboard --logdir='./logs'\n",
        "\n",
        "# 環境・収益の初期化\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "total_episode = 1\n",
        "best_reward = -1\n",
        "\n",
        "start = time.time()\n",
        "for iteration in range(NUM_ITER - cfg.seed_iter):\n",
        "    # ReplayBufferにデータを追加する.\n",
        "    with torch.no_grad():\n",
        "        # 環境と相互作用\n",
        "        action = agent(obs) # (action_dim, )\n",
        "        next_obs, reward, done, _ = env.step(action)\n",
        "        # 得たデータをReplayBufferに追加して更新\n",
        "        replay_buffer.push(preprocess_obs(obs), action, reward, done)\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "\n",
        "    #モデルの学習\n",
        "    if (iteration + 1) % cfg.update_freq == 0:\n",
        "        # ReplayBufferからdataをsamplingする.\n",
        "        # (batch_size, seq_length, *data_shape)\n",
        "        observations, actions, rewards, dones =\\\n",
        "            replay_buffer.sample(cfg.batch_size, cfg.seq_length)\n",
        "        dones = 1 - dones # 終了して無いときは割引率は1にして、終了時割引率0にするので逆にする.\n",
        "        # torchで扱える形に変形\n",
        "        # 各時刻tごとに処理をしたいので、seq lengthを先頭に持ってくる.\n",
        "        observations = torch.permute(torch.as_tensor(observations, device=device), (1, 0, 4, 2, 3)) # (T, B, C, H, W)\n",
        "        actions = torch.as_tensor(actions, device=device).transpose(0, 1) # (T, B, action_dim)\n",
        "        rewards = torch.as_tensor(rewards, device=device).transpose(0, 1) # (T, B, 1)\n",
        "        dones = torch.as_tensor(dones, device=device).transpose(0, 1) # (T, B, 1)\n",
        "\n",
        "        # =================\n",
        "        # world modelの学習\n",
        "        #   画像をすべてEncodeした後,z_t, h_t, z, hを初期化.\n",
        "        #   その後時刻tごとにh, zを埋めて、損失を全体で損失を計算する.\n",
        "        # =================\n",
        "        # すべての観測をベクトルに埋め込み\n",
        "        emb_observations = encoder(observations.reshape(-1, 1, 64, 64)).view(cfg.seq_length, cfg.batch_size, -1) # (T, B, 1536)\n",
        "        # 各時刻tごとのz,hを保存するためのtensorを定義\n",
        "        z = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.z_dim * cfg.n_classes, device=device)\n",
        "        h = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.h_dim, device=device)\n",
        "        # KL lossの計算\n",
        "        kl_loss = 0\n",
        "        for t in range(cfg.seq_length-1):\n",
        "            # hの更新\n",
        "            h[t+1] = rsssm.recurrent(h[t], z[t], actions[t]) # h_t+1\n",
        "            # prior, posteriorのdistribution\n",
        "            prior, detached_prior = rssm.get_prior(h[t+1], detach=True)\n",
        "            posterior, detached_posterior = rssm.get_posterior(h[t+1], emb_observations[t+1], detach=True)\n",
        "            # posteriorからsampling\n",
        "            z[t+1] = posterior.rsample().flatten(1) # (B, z_dim * n_classes)\n",
        "            # KL lossの計算 KL-balancing使ってる\n",
        "            kl_loss += cfg.kl_balance * torch.mean(kl_divergence(detached_posterior, prior)) +\\\n",
        "                       (1 - cfg.kl_balance) * torch.mean(kl_divergence(posterior, detached_prior))\n",
        "        kl_loss /= (cfg.seq_length - 1)\n",
        "\n",
        "        # 初期状態は使わない\n",
        "        h = h[1:] # (seq_length - 1, batch_size, h_dim)\n",
        "        z = z[1:] # (seq_length - 1, batch_size, z_dim * n_classes)\n",
        "        # 得られた状態から再構成・報酬・終端フラグを予測\n",
        "        # そのままでは時間方向、バッチ方向で次元が多いため平坦化\n",
        "        h_flat = h.view(-1, cfg.h_dim) # ((T-1) * B, h_dim)\n",
        "        z_flat = z.view(-1, cfg.z_dim * cfg.n_classes) # ((T-1) * B, z_dim * n_classes)\n",
        "\n",
        "        #再構成、報酬、終端フラグ予測\n",
        "        recon_obs_dist = decoder(h_flat, z_flat) # ((T-1) * B, C, H, W)\n",
        "        reward_dist = reward_model(h_flat, z_flat) # ((T-1) * B, 1)\n",
        "        discount_dist = discount_model(h_flat, z_flat) # ((T-1) * B, 1)\n",
        "\n",
        "        # 各予測に対する対数尤度（損失関数なのでマイナスつける）\n",
        "        C, H, W = observations.shape[2:]\n",
        "        obs_loss = -torch.mean(recon_obs_dist.log_prob(observations[1:].reshape(-1, C, H, W)))\n",
        "        reward_loss = -torch.mean(reward_dist.log_prob(rewards[:-1].reshape(-1, 1)))\n",
        "        discount_loss = -torch.mean(discount_dist.log_prob(dones[:-1].float().reshape(-1, 1)))\n",
        "\n",
        "        # 損失の総和を取る\n",
        "        wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.discount_loss_scale * discount_loss + cfg.kl_loss_scale * kl_loss\n",
        "        # 更新\n",
        "        wm_optimizer.zero_grad()\n",
        "        wm_loss.backward()\n",
        "        clip_grad_norm_(wm_params, cfg.gradient_clipping)\n",
        "        wm_optimizer.step()\n",
        "\n",
        "        # ====================\n",
        "        # Actor, Criticの更新\n",
        "        # ====================\n",
        "\n",
        "        # priorを用いた状態予測\n",
        "        # 格納する空のtensorを用意\n",
        "        imagined_z = torch.zeros(cfg.imagination_horizon + 1,\n",
        "                                 *z_flat.shape,\n",
        "                                 device=z_flat.device) # (imagination_horizon + 1, (T-1) * B, z_dim * n_classes)\n",
        "        imagined_h = torch.zeros(cfg.imagination_horizon + 1,\n",
        "                                 *h_flat.shape,\n",
        "                                 device=h_flat.device) # (imagination_horizon + 1, (T-1) * B, h_dim)\n",
        "        imagined_action_log_probs = torch.zeros(cfg.imagination_horizon,\n",
        "                                                cfg.batch_size * (cfg.seq_length-1),\n",
        "                                                device=h_flat.device) # (imagination_horizon, (T-1) * B)\n",
        "        imagined_action_entropies = torch.zeros(cfg.imagination_horizon,\n",
        "                                                cfg.batch_size * (cfg.seq_length-1),\n",
        "                                                device=h_flat.device) # (imagination_horizon, (T-1) * B)\n",
        "\n",
        "        # 想像上の軌道を作る前に、最初の状態として先ほどReplayBufferからサンプルされた観測データを取り込む\n",
        "        imagined_z[0] = z_flat # ((T-1) * B, z_dim * n_classes)\n",
        "        imagined_h[0] = h_flat # ((T-1) * B, h_dim)\n",
        "\n",
        "        # open-loopで予測\n",
        "        # まず世界モデル上で想像上の状態、行動を取得\n",
        "        # その後それらにすべてに対してまとめて報酬、割引率（終了判定)、状態価値を予測\n",
        "        # 想像上の状態、行動生成\n",
        "        for i in range(cfg.imagination_horizon):\n",
        "            actions, action_log_probs, action_entropies = actor(imagined_h[i], imagined_z[i]) # ((T-1) * B, action_dim or 1)\n",
        "\n",
        "            # hを更新し、priorで次状態を予測\n",
        "            imagined_h[i+1] = rssm.recurrent(imagined_h[i], imagined_z[i], actions)\n",
        "            imagined_z_flat_prior_dist = rssm.get_prior(imagined_h[i+1])\n",
        "            imagined_z[i+1] = imagined_z_flat_prior_dist.rsample().flatten(1)\n",
        "\n",
        "            imagined_action_log_probs[i] = action_log_probs\n",
        "            imagined_action_entropies[i] = action_entropies\n",
        "\n",
        "        imagined_z = imagined_z[1:]\n",
        "        imagined_h = imagined_h[1:]\n",
        "\n",
        "\n",
        "        # 報酬、割引、状態価値の計算\n",
        "        # まとめて報酬、割引を計算するために状態表現h, zの形を変更\n",
        "        imagined_z_flat = imagined_z.view(-1, cfg.z_dim * cfg.n_classes) # (imagination_horizon * (T-1) * B, z_dim * n_classes)\n",
        "        imagined_h_flat = imagined_h.view(-1, cfg.h_dim) # (imagination_horizon * (T-1) * B, h_dim)\n",
        "\n",
        "        # 報酬、割引、状態価値を実際に計算. 計算後形を戻す.\n",
        "        imagined_rewards = reward_model(imagined_h_flat, imagined_z_flat).view(cfg.imagination_horizon, -1) # (imagination_horizon, (T-1) * B)\n",
        "        # targetネットワークを使う\n",
        "        target_values = target_critic(imagined_h_flat, imagined_z_flat).view(cfg.imagination_horizon, -1) # (imagination_horizon, (T-1) * B)\n",
        "        # base_distはIndependentでラップするまえのdistributionを表し、probsはBernoulliの場合pを表す.\n",
        "        imagined_dones = discount_model(imagined_h_flat, imagined_z_flat).base_dist.probs.view(cfg.imagination_horizon, -1) # (imagination_horizon, (T-1) * B)\n",
        "        gamma = cfg.discount * torch.round(imagined_dones) # (imagination_horizon, (T-1) * B)\n",
        "\n",
        "        # lambda_targetの計算\n",
        "        lambda_target = calculate_lambda_target(imagined_rewards, gamma, target_values, cfg.lambda_) # (imagination_horizon, (T-1) * B)\n",
        "\n",
        "        # Actorの損失計算\n",
        "        # 連続値制御ではρ=0が有効（論文より）今回はAtari環境じゃないのでこっちのみ\n",
        "        objective = lambda_target\n",
        "        # lambda_targetは各時刻から見たときの状態価値なので、現在から見たときに変更するため割引率を掛ける\n",
        "        gamma = torch.cat([torch.ones_like(gamma[:1]), gamma[1:]])\n",
        "        gamma = torch.cumprod(gamma, 0)\n",
        "        actor_loss = -torch.sum(torch.mean(gamma * (objective + cfg.actor_entropy_scale * imagined_action_entropies), dim=1))\n",
        "\n",
        "        actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        clip_grad_norm_(actor.parameters(), cfg.gradient_clipping)\n",
        "        actor_optimizer.step()\n",
        "\n",
        "        # Criticの損失計算\n",
        "        # 論文内ではMSEだけど実装では負の対数尤度を損失関数にしている.\n",
        "        value_mean = critic(imagined_h_flat, imagined_z_flat).view(cfg.imagination_horizon, -1)\n",
        "        value_dist = td.Independent(Normal(value_mean, 1), 1)\n",
        "        critic_loss = -torch.mean(gamma.detach() * value_dist.log_prob(lambda_target.detach()).unsqueeze(-1))\n",
        "\n",
        "        critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        clip_grad_norm_(critic.parameters(), cfg.gradient_clipping)\n",
        "        critic_optimizer.step()\n",
        "\n",
        "    # targetの更新\n",
        "    if (iteration + 1) % cfg.slow_critic_update == 0:\n",
        "        target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "    # エピソードが終了したときに初期化\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        total_episode += 1\n",
        "        agent.reset()\n",
        "\n",
        "    # 一定iterationごとに評価\n",
        "    if (iteration + 1) % cfg.eval_freq == 0:\n",
        "        agents_h_clone = agent.h.clone()\n",
        "        eval_best_reward, eval_mean_reward = evaluation(eval_env, agent, iteration, cfg)\n",
        "        trained_models.save(\"./\")\n",
        "        if eval_best_reward > best_reward:\n",
        "            best_reward = eval_best_reward\n",
        "            os.makedirs(\"./best_model\", exist_ok=True)\n",
        "            trained_models.save(\"./best_model\")\n",
        "        writer.add_scalar('eval_mean_reward', eval_mean_reward, iteration)\n",
        "        print(f\"iterations: {iteration}[{(iteration+1)/1e3}/{(NUM_ITER - cfg.seed_iter)/1e3}]({time.time() - start}s) eval_mean_reward: {eval_mean_reward:.8f}\")\n",
        "        start = time.time()\n",
        "        print(f\"obs loss: {obs_loss.item():.8f}\"\n",
        "              f\"reward loss: {reward_loss.item():.8f}\"\n",
        "              f\"discount loss: {discount_loss.item():.8f}\"\n",
        "              f\"kl loss: {kl_loss.item():.8f}\"\n",
        "              f\"actor loss: {actor_loss.item():.8f}\"\n",
        "              f\"critic loss: {critic_loss.item():.8f}\"\n",
        "              f\"total_episode: {total_episode}\")\n",
        "        writer.add_scalar('eval rewards', eval_reward, iteration)\n",
        "        writer.add_scalar('obs loss', obs_loss.item(), iteration)\n",
        "        writer.add_scalar('reward loss', reward_loss.item(), iteration)\n",
        "        writer.add_scalar('discount loss', discount_loss.item(), iteration)\n",
        "        writer.add_scalar('kl loss', kl_loss.item(), iteration)\n",
        "        writer.add_scalar('actor loss', actor_loss.item(), iteration)\n",
        "        writer.add_scalar('critic loss', critic_loss.item(), iteration)\n",
        "\n",
        "        eval_env.reset()\n",
        "        agent.h = agents_h_clone\n",
        "\n",
        "trained_models.save(\"./\")\n"
      ],
      "metadata": {
        "id": "5BEoYGgex0SL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}